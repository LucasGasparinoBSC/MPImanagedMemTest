# mpiManagedMemTest

Simple project for testing the managed memory feature of MPI + OpenACC.

## Requirements

* OpenACC compiler
* MPI library
* CMake version 3.15 or higher

Both are best fulfilled by using the NVIDIA HPC SDK (https://developer.nvidia.com/hpc-sdk) suite. CMake is not mandatory, but will make life easier.

## Building

The easy way uses CMake to generate the build files. The following commands will generate build files assuming you are using the NVIDIA HPC SDK with OpenACC:

```bash
mkdir build
cd build
cmake ..
```

To test without the OpenACC features, use the following command:

```bash
cmake -DUSE_ACC=OFF ..
```

To build without CMake, use the following commands:

```bash
mpicxx -O3 -cuda -lnvToolsExt -Minfo=all [-acc -gpu=ccXY,managed] -o mpiManagedMemTest mpiManagedMemTest.cpp
```

The options in "[]" are optional. The "-acc" option enables OpenACC, and the "-gpu=ccXY,managed" option enables managed memory. The "ccXY" option should be replaced with the compute capability of the GPU you are using. For example, if you are using a Tesla V100, use "-gpu=cc70,managed".

## Running

The program can be run with the following command:

```bash
mpirun -np [N] ./[execName]
```

The "execName" should be replaced with the name of the executable generated by the build process. The "-np" option should be replaced with the number of processes you want to run. If running with OpenACC, the number of processes should be equal to the number of GPUs you have available, though this is not a requirement.
As is configured, the amount of memory allocated per process is ~3 GB. This can be changed by modifying the "arrSize" variable in the "main" function of "mpiTest.cpp". Using more ranks increases the total required memory, so be sure to have enough memory available on the system.